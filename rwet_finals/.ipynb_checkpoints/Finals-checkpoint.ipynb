{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "f23f1a6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in /Users/pien/anaconda3/lib/python3.11/site-packages (0.9.4)\r\n",
      "Requirement already satisfied: unidecode in /Users/pien/anaconda3/lib/python3.11/site-packages (from markovify) (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import markovify\n",
    "\n",
    "!{sys.executable} -m pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db9521",
   "metadata": {},
   "source": [
    "One day I visited MOMA with a friend, and we talked about Le Corbusier’s architecture, The Connector musical, Grayson’s Art Club, low-tech mini games and Pope Francis’s fashion sense. Our conversation also came upon Oulipo (Ouvroir de littérature potentielle, Workshop of Potential Literature), and we were both really fascinated by the works this collective had made. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349d959",
   "metadata": {},
   "source": [
    "I was super interested by the idea of a bunch of writers and mathmeticians gathering together and discuss literature, and write poetry.\n",
    "\"freeing literature by tightening its rules\" hmmmm yea I guess I agree with this concept. A lot of times when I create stuff, having a prompt is almost always better than just handing my a blank peice of paper.\n",
    "I do need atleast some sort of starting point to start with!\n",
    "\n",
    "These people do really like a challenge though!\n",
    "Here are some common ones:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    the Oulipo Constraints\n",
    "                                    ----------------------\n",
    "   - Lipogram: Writing that excludes one or more letters\n",
    "   - N+7: Replacing each noun in a text with the noun found seven entries after it in a dictionary.\n",
    "   - Snowball: Doubling the length of each word in a text.\n",
    "   - S+7: Replacing each substantive noun in a text with the noun found seven entries after it in a dictionary.\n",
    "   - Palindrome: Creating sentences or words that read the same forwards and backwards, such as \"A man, a plan, a canal: Panama.\"\n",
    "   - Prisoner's Constraint: Writing without using the letter \"e\" more often than once in a word.\n",
    "   - Tautogram: Creating a text where all words start with the same letter.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1ff3f",
   "metadata": {},
   "source": [
    "This to me just looks very computational, them coming up with this concept and doing this manually is pretty nuts to me. But I do think it's interesting want give it a try and see what would happen.\n",
    "\n",
    "The source text I want to use is The Beatrice Letters by Lemony Snicket(Daniel Handler).\n",
    "I thought, and still think, that his work is fun to read because his way of describing things is really random but hits the right spot for me at the same time. They’re dream-like and impish and all over the place, like he doesn’t seem to have a preference for words basically. Even back in me as a ten/eleven-year-old kid reading the A Series of Unfortunate Events, his style was very distinguishable and special. I would even say that his style is sort of generative-like, because it’s so random haha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "8a9753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def gen_from_model(n, model, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_item = random.choice(model[start])\n",
    "        if next_item is None:\n",
    "            break\n",
    "        else:\n",
    "            output.append(next_item)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "7f75faf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all, and I will love you no matter how many fire drills we are all forced to endure, and no matter how your soccer team performed in the hairdos of the people who talk too much. I will love you no matter how your soccer team performed in the hopes of catching a burglar, and as an airplane loves to leave dust in the hopes of catching a burglar, and as a daggerproof tunic loves to shatter on the floor instead of hanging it up and I will love you as the starfish loves a coral reef and as a cloud loves bats and as justice loves to fall from a clear blue sky and as someone else gasping loves a secret, and as a range loves braes. I will love you no matter how many mistakes I make when trying to reduce fractions, and no matter how hard I try. I will love you if you have two children, or three children, or even O., or anyone Z. through A., even R. although sadly I believe it will be quite some time before two women can be allowed to marry – and I will love you if you don’t see, and\n"
     ]
    }
   ],
   "source": [
    "genesis_word_model = train_markov_chain(open(\"iwillloveyou.txt\", encoding='utf8').read(), order=2)\n",
    "generated_words = gen_from_model(2, genesis_word_model, max_gen=200)\n",
    "print(' '.join(generated_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a700c",
   "metadata": {},
   "source": [
    "Lipogram attempt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "id": "ec6169ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discover to you,\n",
      "loves the crumbs,\n",
      "foggy fog. I will\n",
      "of every clock\n"
     ]
    }
   ],
   "source": [
    "#no letter \"a\"\n",
    "import random\n",
    "\n",
    "def without_letter(n, model, letter, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_word_candidates = [word for word in model.get(start, []) if letter not in word]\n",
    "        if not next_word_candidates:\n",
    "            break\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        output.append(next_word)\n",
    "    output = [word for word in output if letter not in word]\n",
    "    return output\n",
    "\n",
    "genesis_word_model = train_markov_chain(open(\"iwillloveyou.txt\", encoding='utf8').read(), order=3)\n",
    "\n",
    "for i in range(4):\n",
    "    without_a = without_letter(3, genesis_word_model, 'a', max_gen=500) \n",
    "    print(' '.join(without_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93484dc",
   "metadata": {},
   "source": [
    "    memory, your\n",
    "    how I discover\n",
    "    loves off\n",
    "    of others. I will\n",
    "    \n",
    "    discover to you,\n",
    "    loves the crumbs,\n",
    "    foggy fog. I will\n",
    "    of every clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "id": "a361723c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y., or even O., or\n",
      "to me happens to\n",
      "your father. I\n",
      "betray your father. I\n"
     ]
    }
   ],
   "source": [
    "#no letter \"i\"\n",
    "import random\n",
    "\n",
    "def without_letter(n, model, letter, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_word_candidates = [word for word in model.get(start, []) if letter not in word]\n",
    "        if not next_word_candidates:\n",
    "            break\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        output.append(next_word)\n",
    "    output = [word for word in output if letter not in word]\n",
    "    return output\n",
    "\n",
    "genesis_word_model = train_markov_chain(open(\"iwillloveyou.txt\", encoding='utf8').read(), order=3)\n",
    "\n",
    "for i in range(4):\n",
    "    without_i = without_letter(3, genesis_word_model, 'i', max_gen=500) \n",
    "    print(' '.join(without_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e2905",
   "metadata": {},
   "source": [
    "    love the and\n",
    "    flavor of naval\n",
    "    how \n",
    "    your years you had\n",
    "    \n",
    "    ourselves farther and\n",
    "    you and who you don’t\n",
    "\n",
    "    sadly \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "c484fcde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as I am discovering this. I will love\n",
      "shattering loves to make a person gasp, and as a gasping person loves a glass of\n",
      "people who talk too much. I will love\n",
      "take up the harmonica and I will love\n"
     ]
    }
   ],
   "source": [
    "#no letter \"y\"\n",
    "import random\n",
    "\n",
    "def without_letter(n, model, letter, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_word_candidates = [word for word in model.get(start, []) if letter not in word]\n",
    "        if not next_word_candidates:\n",
    "            break\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        output.append(next_word)\n",
    "    output = [word for word in output if letter not in word]\n",
    "    return output\n",
    "\n",
    "genesis_word_model = train_markov_chain(open(\"iwillloveyou.txt\", encoding='utf8').read(), order=3)\n",
    "\n",
    "for i in range(4):\n",
    "    without_y = without_letter(3, genesis_word_model, 'y', max_gen=500) \n",
    "    print(' '.join(without_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bf5bb",
   "metadata": {},
   "source": [
    "    loves to drop from its shirt and explore the\n",
    "    drawn upon the blackboard in a blurring, boring chalk. I will love\n",
    "    have a child, and I will love\n",
    "    realizing that she has left her dagger at home. I will love\n",
    "    \n",
    "    men and as peppermints love\n",
    "    the world goes on its wicked\n",
    "    off a house on a\n",
    "    someone else –\n",
    "    \n",
    "    reveal a secret compartment. I will love\n",
    "    I make when\n",
    "    the actions of our enemies or the jealousies of actors. I will love\n",
    "    no regard to the actions of our enemies or the jealousies of actors. I will love\n",
    "    \n",
    "    -----------------------------------\n",
    "    \n",
    "    the memorized\n",
    "    and until the apple hates a nest, although\n",
    "    \n",
    "    horseradish loves the\n",
    "    see\n",
    "    \n",
    "    even more, although I\n",
    "    of a clock. I will love\n",
    "    \n",
    "    Love You.\" Letter. The Beatrice Letters,\n",
    "    spoon, between our lips and fingers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2bfc7",
   "metadata": {},
   "source": [
    "Now omitting the letter \"e\", just like how Ernest Vincent Wright wrote Gadsby in 1939 and Georges Perec wrote La Disparition in 1969."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "804581f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bats and as a crow\n",
      "frogs to M. I will\n",
      "which has just\n",
      "Edgar is\n"
     ]
    }
   ],
   "source": [
    "#no letter \"e\"\n",
    "import random\n",
    "\n",
    "def without_letter(n, model, letter, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_word_candidates = [word for word in model.get(start, []) if letter not in word]\n",
    "        if not next_word_candidates:\n",
    "            break\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        output.append(next_word)\n",
    "    output = [word for word in output if letter not in word]\n",
    "    return output\n",
    "\n",
    "genesis_word_model = train_markov_chain(open(\"iwillloveyou.txt\", encoding='utf8').read(), order=3)\n",
    "\n",
    "for i in range(4):\n",
    "    without_e = without_letter(3, genesis_word_model, 'e', max_gen=500)\n",
    "    print(' '.join(without_e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675ff11",
   "metadata": {},
   "source": [
    "    of his and N.\n",
    "    until all\n",
    "    and crumbs\n",
    "    of glass\n",
    "    \n",
    "    as a taxi\n",
    "    you if you marry\n",
    "    Z. through A.,\n",
    "    in\n",
    "    \n",
    "    a glass of brandy to calm\n",
    "    as\n",
    "    to wind and\n",
    "    as a child\n",
    "    \n",
    "    catching a burglar, and as a library\n",
    "    miyagi, as\n",
    "    ship, and\n",
    "    uniform. I will\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0bc7f",
   "metadata": {},
   "source": [
    "These are some I like and have put them into a four stanza poem. \n",
    "I think this is interesting, and I want to continue using the lipogram constraint and write a love letter in the format of The Beatrice Letters.\n",
    "Let me first switch out all the nouns, verbs, adjectives and adverbs of TBL with G's wordbank. But I'm keeping \"I\" and changing the word \"love\" to \"miss\" so it's still romantic.\n",
    "\n",
    "So, randomly selecting a n/v/adj/adv for replacement, tokenize the text into words, and tagging each word with its part of speech(POS)*. I will create an empty list to store the G words and perform the process of POS on each of them and join them back to a string. \n",
    "\n",
    "\n",
    "**note** part of speech(POS): analyzing each word in the text and determining its grammatical category, such as noun, verb, adjective, adverb, etc. The NLTK library provides a function called pos_tag that can perform this task. After tagging, each word is represented as a tuple containing the word itself and its corresponding POS tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "1c6b259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacement complete. Check 'iwillloveyou_modified.txt' for the modified text.\n"
     ]
    }
   ],
   "source": [
    "with open(\"iwillloveyou.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "modified_text = text.replace(\"love\", \"miss\")\n",
    "\n",
    "with open(\"iwillloveyou_modified.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(modified_text)\n",
    "\n",
    "print(\"Replacement complete. Check 'iwillloveyou_modified.txt' for the modified text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ec13b",
   "metadata": {},
   "source": [
    "With this modification, all the \"love\" are now miss, which is great, but another problem is now there are also \"misss\",\n",
    "which I think is suppose to be misses, and now I'm just going to modify it again, changing \"misss\" to \"holds\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "124a0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I Will Love You.\" Letter. The Beatrice Letters, Lemony Snicket\n",
      "Always. Continuously. With increasing apprehension, and decreasing hope.\n",
      "\n",
      "I will miss you with no regard to the actions of our enemies or the jealousies of actors. I will miss you with no regard to the outrage of certain parents or the boredom of certain friends. I will miss you no matter what is served in the world’s cafeterias or what game is played at each and every recess. I will miss you no matter how many fire drills we are all forced to endure, and no matter what is drawn upon the blackboard in a blurring, boring chalk. I will miss you no matter how many mistakes I make when trying to reduce fractions, and no matter how difficult it is to memorize the periodic table. I will miss you no matter what your locker combination was, or how you decided to spend your time during study hall. I will miss you no matter how your soccer team performed in the tournament or how many stains I received on my cheerleading uniform. I will miss you if I never see you again, and I will miss you if I see you every Tuesday. I will miss you if you cut your hair and I will miss you if you cut the hair of others. I will miss you if you abandon your baticeering, and I will miss you if you retire from the theater to take up some other, less dangerous occupation. I will miss you if you drop your raincoat on the floor instead of hanging it up and I will miss you if you betray your father. I will miss you even if you announce that the poetry of Edgar Guest is the best in the world and even if you announce that the work of Zilpha Keatley Snyder is unbearably tedious. I will miss you if you abandon the theremin and take up the harmonica and I will miss you if you donate your marmosets to the zoo and your tree frogs to M. I will miss you as the starfish holds a coral reef and as kudzu holds trees, even if the oceans turn to sawdust and the trees fall in the forest without anyone around to hear them. I will miss you as the pesto holds the fetuccini and as the horseradish holds the miyagi, as the tempura holds the ikura and the pepperoni holds the pizza. I will miss you as the manatee holds the head of lettuce and as the dark spot holds the leopard, as the leech holds the ankle of a wader and as a corpse holds the beak of the vulture. I will miss you as the doctor holds his sickest patient and a lake holds its thirstiest swimmer. I will miss you as the beard holds the chin, and the crumbs miss the beard, and the damp napkin holds the crumbs, and the precious document holds the dampness in the napkin, and the squinting eye of the reader holds the smudged print of the document, and the tears of sadness miss the squinting eye as it misreads what is written. I will miss you as the iceberg holds the ship, and the passengers miss the lifeboat, and the lifeboat holds the teeth of the sperm whale, and the sperm whale holds the flavor of naval uniforms. I will miss you as a child holds to overhear the conversations of its parents, and the parents miss the sound of their own arguing voices, and as the pen holds to write down the words these voices utter in a notebook for safekeeping. I will miss you as a shingle holds falling off a house on a windy day and striking a grumpy person across the chin, and as an oven holds malfunctioning in the middle of roasting a turkey. I will miss you as an airplane holds to fall from a clear blue sky and as an escalator holds to entangle expensive scarves in its mechanisms. I will miss you as a wet paper towel holds to be crumpled into a ball and thrown at a bathroom ceiling and an eraser holds to leave dust in the hairdos of the people who talk too much. I will miss you as a cufflink holds to drop from its shirt and explore the party for itself and as a pair of white gholds holds to slip delicately into the punchbowl. I will miss you as a taxi holds the muddy splash of a puddle and as a library holds the patient tick of a clock. I will miss you as a thief holds a gallery and as a crow holds a murder, as a cloud holds bats and as a range holds braes. I will miss you as misfortune holds orphans, as fire holds innocence and as justice holds to sit and watch while everything goes wrong. I will miss you as a battlefield holds young men and as peppermints miss your allergies, and I will miss you as the banana peel holds the shoe of a man who was just struck by a shingle falling off a house. I will miss you as a volunteer fire department holds rushing into burning buildings and as burning buildings miss to chase them back out, and as a parachute holds to leave a blimp and as a blimp operator holds to chase after it. I will miss you as a dagger holds a certain person’s back, and as a certain person holds to wear daggerproof tunics, and as a daggerproof tunic holds to go to a certain dry cleaning facility, and how a certain employee of a dry cleaning facility holds to stay up late with a pair of binoculars, watching a dagger factory for hours in the hopes of catching a burglar, and as a burglar holds sneaking up behind people with binoculars, suddenly realizing that she has left her dagger at home. I will miss you as a drawer holds a secret compartment, and as a secret compartment holds a secret, and as a secret holds to make a person gasp, and as a gasping person holds a glass of brandy to calm their nerves, and as a glass of brandy holds to shatter on the floor, and as the noise of glass shattering holds to make someone else gasp, and as someone else gasping holds a nearby desk to lean against, even if leaning against it presses a lever that holds to open a drawer and reveal a secret compartment. I will miss you until all such compartments are discovered and opened, and until all the secrets have gone gasping into the world. I will miss you until all the codes and hearts have been broken and until every anagram and egg has been unscrambled. I will miss you until every fire is extinguished and until every home is rebuilt form the handsomest and most susceptible of woods, and until every criminal is handcuffed by the laziest of policemen. I will miss you until M. hates snakes and J. hates grammar, and I will miss you until C. realizes S. is not worthy of his miss and N. realizes he is not worthy of the V. I will miss you until the bird hates a nest and the worm hates an apple, and until the apple hates a tree and the tree hates a nest, and until a bird hates a tree and an apple hates a nest, although honestly I cannot imagine that last occurrence no matter how hard I try. I will miss you as we grow older, which has just happened, and has happened again, and happened several days ago, continuously, and then several years before that, and will continue to happen as the spinning hands of every clock and the flipping pages of every calendar mark the passage of time, except for the clocks that people have forgotten to wind and the calendars that people have forgotten to place in a highly visible area. I will miss you as we find ourselves farther and farther from one another, where once we were so close that we could slip the curved straw, and the long, slender spoon, between our lips and fingers respectively. I will miss you until the chances of us running into one another slip from skim to zero, and until your face is fogged by distant memory, and your memory faced by distant fog, and your fog memorized by a distant face, and your distance distanced by the memorized memory of a foggy fog. I will miss you no matter where you go and who you see, no matter where you avoid and who you don’t see, and no matter who sees you avoiding where you go. I will miss you no matter what happens to you, and no matter how I discover what happens to you, and no matter what happens to me as I discover this, and no matter how I am discovered after what happens to me happens to me as I am discovering this. I will miss you if you don’t marry me. I will miss you if you marry someone else – your co-star, perhaps, or Y., or even O., or anyone Z. through A., even R. although sadly I believe it will be quite some time before two women can be allowed to marry – and I will miss you if you have a child, and I will miss you if you have two children, or three children, or even more, although I personally think three is plenty, and I will miss you if you never marry at all, and never have children, and spend your years wishing you had married me after all, and I must say that on late, cold nights I prefer this scenario out of all the scenarios I have mentioned. That, Beatrice, is how I will miss you even as the world goes on its wicked way.\n"
     ]
    }
   ],
   "source": [
    "with open(\"iwillloveyou_modified.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    modified_text = file.read()\n",
    "\n",
    "modified_text = modified_text.replace(\"misss\", \"holds\")\n",
    "\n",
    "with open(\"iwillloveyou_modified.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(modified_text)\n",
    "\n",
    "with open(\"iwillloveyou_modified.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    modified_text = file.read()\n",
    "\n",
    "    print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "a1af1f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will miss you with no part to the pals of our hands or the Council of Simpkins . I will miss you with no pass to the Bill of public population or the Branton of tiny citywards . I will miss you no passing what coming knows in the pay — onyx gymnasiums or what visitors took bury at each and every Gadsby . I will miss you no XIV how tiny band-stand spot we casting all going to had , and no Nancy what took march upon the Julius in a blasts , living lamp . I will miss you no call how tiny vacancy I know when thought to sitting conduct , and no notion how old it saw to struck the full-grown Standish . I will miss you no dish what your child band took , or how you did to swoop your mark during croon Organization . I will miss you no squash how your oratory Training call in the Lady or how occasional sanctity I worrying on my Dad Christmas . I will miss you if I soon had you out , and I will miss you if I is you every lawn . I will miss you if you found your thralldom and I will miss you if you was the shook of clothing . I will miss you if you got your town , and I will miss you if you sobbing from the door to is slowly some grand , almost youthful Grammar . I will miss you if you running your Bill on the Four off of turn it up and I will miss you if you having your Branton . I will miss you out if you was that the March of Clancy wisp has the criss-cross in the nothing and always if you Rushing that the Nancy of finds Arthur burst 's back many . I will miss you if you burst the Old and was Only the Arthur and I will miss you if you arguing your night to the bit and your boys Lucy to Utopia I will miss you as the human shouting a unfamiliar Ability and as young shown Kathlyn , n't if the Madam was to hustling and the comfort brought in the thousand without sit around to was them . I will miss you as the today got the tots and as the big crawl the history , as the skull 's the insomnia and the salad doing the childhood . I will miss you as the City smoking the stamp of look and as the ignorant topic talk the instruction , as the slyly digging the fight of a troop and as a Hills was the condition of the man . I will miss you as the John obstruct his old journalism and a kind and—— its nursing Youth . I will miss you as the animals was the Morgan , and the Branton miss the graft , and the surprising Norman find the hour , and the amazing cub-hood mowing the —no in the Gadsby , and the doorway animals of the girl is the old things of the ballots , and the candidacy of — miss the finding thousands as it was what 's shining . I will miss you as the Man say the armor—and , and the farm miss the guys , and the clothing stirs the — of the such spots , and the customary facility known the pig-tails of many nursing . I will miss you as a fond going to do the bright of its School , and the hair miss the Antor of their good Hills hand , and as the apt occur to amount now the fact these aids fill in a mail for did . I will miss you as a Young got running actually a pupil on a following family and is a such So across the law , and as an happy was waiting in the group of run a Bill . I will miss you as an Broadway drop to coming from a high-sounding thoughts anticipation and as an cigar looking to think old Harold in its plums . I will miss you as a clown Branton Hospital wish to go did into a today and 's at a district Trucks and an XIV including to snoozing Nancy in the transformation of the cash who stop just much . I will miss you as a parks saying to find from its Nancy and said the products for itself and as a Man of young picnic according to was unlawfully into the Virginia . I will miss you as a bid allow the public information of a son and as a hot do the many months of a fall . I will miss you as a ironing wanting a Gadsby and as a contains coming a Gadsby , as a shop is star and as a brain know braes . I will miss you as light hit cap , as anything looking sparks and as girls holding to wish and looking while grin frying many . I will miss you as a brow waiting droom-stick Councilman and as look miss your goods , and I will miss you as the sir lodging grab the glass of a trip who had down sought by a film knows back a pity . I will miss you as a scarfs throat —is find find into chuckling school and as work law miss to find them kindly out , and as a bit was to did a grandchild and as a Bill Standish find to has after it . I will miss you as a battalions had a good XVII Mayor swain as , and as a own child is to do tallow-dip gift , and as a woman back fight to took to a small good wars Did , and how a such thinking-porch of a many Gadsby months is to trying Now Now with a Start of bunch , got a worth HISTORY for folks in the lady of walk a Branton , and as a vitalic looking playing so behind grabs with City , continuously do that she mocking has her tip at sun . I will miss you as a stop had a Gadsby girl , and as a now-a-days glory say a big , and as a natural was to stick a racks * , and as a think history 's a campaign of spirit to has their bunch , and as a sword of mixing giving to hanging on the City , and as the knowing of Honor know boast to did Virginia almost gift , and as school n't growing put a brown girl to bring against , only if surrounding against it hook a San that putting to going a glass-blowing and coming a tall — . I will miss you until all daily act rushing rolling and fight , and until all the quality stay studying doubling into the Gadsby . I will miss you until all the Antor and —nights do occur 's and until every tying and knows growing donating is . I will miss you until every Marian contain had and until every Branton was physical pair the happy and right big of grip , and until every man-of-all-work unpacking is by the happy of cotton . I will miss you until Black kill — and world grab Grammar , and I will miss you until hand 's Gadsby was out prosaic of his miss and doorway was he do long additional of the Lucy I will miss you until the Julius approaching a kick and the Twilight said an assistants , and until the May function a lunch and the room do a Zoo , and until a hand put a Organization and an Hills do a just , although down I can still pays that now-a-days strands no minds how various I avoiding . I will miss you as we Do gigantic , which rushing back has , and laying grow only , and said big jargon so , up , and happily high-swinging school before that , and will was to drawn as the was Gadsby of every balloon and the boyish points of every hour victory the Branton of Branton , except for the mind that fund sobbing did to boiling and the thousand that band did is to Is in a so famous winds . I will miss you as we wish ourselves just and far from one another , where just we grow Family worthy that we could was the many Donaldson , and the municipal , complaining band , between our coulda and pond up . I will miss you until the girl of us taking into one another shadowy from girl to zero , and until your Gadsby know outbidding by poor kids , and your man-of-all-work riding by big plants , and your war is by a big food , and your dug is by the artistic thousands of a calm Gadsby . I will miss you no clamor where you is and who you land , no gnus where you found and who you try such animal shins , and no calm who built you stand where you XXXVIII . I will miss you no Christmas what fly to you , and no Mary how I sang what lugging to you , and no group what go to me as I is this , and no form how I go told after what starts to me taught to me as I 's stop this . I will miss you if you was half goal Old me . I will miss you if you was fuss still rang your Lucy , now , or School , or Finally party , or Can plain through room , just Branton although only I think it will call as some colts before two Dan can attain is to Think days and I will miss you if you was a Branton , and I will miss you if you satisfy two Natural , or three town , or So low , although I along say three ran awful , and I will miss you if you Now dropping at all , and still was funds , and wait your talk fight you was was me after all , and I must mind that on tiny , small pal I said this drinking out of all the opportunity I watching took . That , zoo , going how I will miss you soon as the Youth Calling on its only work .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def replace_word(word, tag, gadsby_tagged_words):\n",
    "    if word.lower() in [\"i\", \"miss\", \"misss\"]:\n",
    "        return word\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        new_word = random.choice([word for word, tag in gadsby_tagged_words if tag.startswith('N')])\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        new_word = random.choice([word for word, tag in gadsby_tagged_words if tag.startswith('V')])\n",
    "    elif tag.startswith('J'):  # Adjective\n",
    "        new_word = random.choice([word for word, tag in gadsby_tagged_words if tag.startswith('J')])\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        new_word = random.choice([word for word, tag in gadsby_tagged_words if tag.startswith('R')])\n",
    "    else:\n",
    "        # Keep the rest unchanged\n",
    "        new_word = word\n",
    "    return new_word\n",
    "\n",
    "with open(\"iwillmissyou.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    iwillmissyou_text = file.read()\n",
    "with open(\"Gadsby.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    gadsby_text = file.read()\n",
    "\n",
    "#Tokenize\n",
    "iwillmissyou_words = nltk.word_tokenize(iwillmissyou_text)\n",
    "gadsby_words = nltk.word_tokenize(gadsby_text)\n",
    "\n",
    "#Tag each word with its POS\n",
    "iwillmissyou_tagged_words = nltk.pos_tag(iwillmissyou_words)\n",
    "gadsby_tagged_words = nltk.pos_tag(gadsby_words)\n",
    "\n",
    "#empty list to store\n",
    "modified_words = []\n",
    "\n",
    "for word, tag in iwillmissyou_tagged_words:\n",
    "    new_word = replace_word(word, tag, gadsby_tagged_words)\n",
    "    modified_words.append(new_word)\n",
    "\n",
    "modified_text = ' '.join(modified_words)\n",
    "\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e1de0",
   "metadata": {},
   "source": [
    "I think this pretty much filters out most of the words with \"e\", and is good enough for me to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "392e6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_model(model, n, seq):\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]            \n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)\n",
    "\n",
    "def markov_model(n, seq):\n",
    "    model = {}\n",
    "    add_to_model(model, n, seq)\n",
    "    return model\n",
    "\n",
    "poem_markov_model = markov_model(2, open(\"iwillmissyou_Gadsbyversion.txt\", encoding='utf8').read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "id": "4380069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will miss you until every criminal is handcuffed by the memorized memory of a puddle and as the horseradish holds the head of lettuce and as an airplane holds to sit and watch while everything goes wrong. \n",
      "\n",
      "I will miss you if I see you every Tuesday. \n",
      "\n",
      "I will miss you no matter where you go. \n",
      "\n",
      "I will miss you if you don’t marry me. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def gen_from_model(order, model, start, max_gen=100):\n",
    "    generated = list(start)\n",
    "    for _ in range(max_gen):\n",
    "        prefix = tuple(generated[-order:])\n",
    "        if prefix in model:\n",
    "            next_word = random.choice(model[prefix])\n",
    "            generated.append(next_word)\n",
    "            \n",
    "            # Check if the next word is a period\n",
    "            if next_word.endswith('.'):\n",
    "                # Insert a line break after the period\n",
    "                generated.append('\\n')\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return generated\n",
    "\n",
    "sentences = []\n",
    "for i in range(4):\n",
    "    generated_words = gen_from_model(2, poem_markov_model, ('I', 'will'))\n",
    "    sentences.append(' '.join(generated_words))\n",
    "\n",
    "print('\\n'.join(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "id": "abf73068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "periodic table. I will miss you until M.\n",
      "miss you as a taxi holds\n",
      "the harmonica and I will miss you if you drop your raincoat on\n",
      "puddle and as a crow holds a\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def train_markov_chain(text, order):\n",
    "    model = {}\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - order):\n",
    "        prefix = tuple(words[i:i+order])\n",
    "        suffix = words[i+order]\n",
    "        if 'e' not in suffix:  # Skip words containing 'e'\n",
    "            if prefix in model:\n",
    "                model[prefix].append(suffix)\n",
    "            else:\n",
    "                model[prefix] = [suffix]\n",
    "    return model\n",
    "\n",
    "def generate_text(model, start_prefix, max_gen=50):\n",
    "    generated = list(start_prefix)\n",
    "    for _ in range(max_gen):\n",
    "        prefix = tuple(generated[-order:])\n",
    "        if prefix in model:\n",
    "            next_word = random.choice(model[prefix])\n",
    "            generated.append(next_word)\n",
    "            if len(generated) >= 10 and next_word.endswith(('.', ',', ';')):\n",
    "                break  # Break if we encounter a punctuation mark and the sentence is sufficiently long\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Read the text from the file and train the Markov chain model\n",
    "order = 3\n",
    "with open(\"iwillmissyou_Gadsbyversion.txt\", encoding='utf8') as file:\n",
    "    text = file.read()\n",
    "genesis_word_model = train_markov_chain(text, order)\n",
    "\n",
    "# Generate and print four different short sentences\n",
    "for _ in range(4):\n",
    "    start_prefix = random.choice(list(genesis_word_model.keys()))\n",
    "    sentence = generate_text(genesis_word_model, start_prefix, max_gen=30)\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0109641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a8c657c",
   "metadata": {},
   "source": [
    "ones I like:\n",
    "\n",
    "    Gadsby of every Florist and\n",
    "    too we aid too grown-up that\n",
    "    I miss\n",
    "\n",
    "    and as peppermints miss your allergies, and I will miss you as a cufflink holds to drop from its shirt and explore the party for itself and as a corpse holds the beak of the vulture. \n",
    "\n",
    "    as the noise of glass shattering holds to make a person gasp, and as someone else gasping holds a nearby desk to lean against, even if leaning against it presses a lever that holds to open a drawer and reveal a secret compartment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4a06f",
   "metadata": {},
   "source": [
    "    if you betray your father. \n",
    "\n",
    "    in a notebook for safekeeping. \n",
    "    \n",
    "    memorized memory of a foggy fog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d449916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
